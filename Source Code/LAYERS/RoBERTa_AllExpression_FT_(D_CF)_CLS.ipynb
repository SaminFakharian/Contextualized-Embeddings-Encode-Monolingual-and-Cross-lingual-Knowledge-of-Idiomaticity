{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"RoBERTa_AllExpression_FT_(D_CF)_CLS.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.5"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"69ZcIDFJny21","executionInfo":{"status":"ok","timestamp":1607995640121,"user_tz":240,"elapsed":7554,"user":{"displayName":"Samin Fakharian","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjFi1gkXRQ0jRQiMMLiILHLVk8ulyIY9ehIbVkQ5g=s64","userId":"00770828924006406210"}},"outputId":"d103f3ed-d23d-46db-da51-fd537f22b3b1"},"source":["!pip install transformers==3.0.0"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Collecting transformers==3.0.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9c/35/1c3f6e62d81f5f0daff1384e6d5e6c5758682a8357ebc765ece2b9def62b/transformers-3.0.0-py3-none-any.whl (754kB)\n","\u001b[K     |████████████████████████████████| 757kB 12.3MB/s \n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.0) (4.41.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.0) (2019.12.20)\n","Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.0) (2.23.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.0) (20.7)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.0) (3.0.12)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.0) (1.18.5)\n","Collecting tokenizers==0.8.0-rc4\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e8/bd/e5abec46af977c8a1375c1dca7cb1e5b3ec392ef279067af7f6bc50491a0/tokenizers-0.8.0rc4-cp36-cp36m-manylinux1_x86_64.whl (3.0MB)\n","\u001b[K     |████████████████████████████████| 3.0MB 30.6MB/s \n","\u001b[?25hRequirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.0) (0.8)\n","Collecting sentencepiece\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e5/2d/6d4ca4bef9a67070fa1cac508606328329152b1df10bdf31fb6e4e727894/sentencepiece-0.1.94-cp36-cp36m-manylinux2014_x86_64.whl (1.1MB)\n","\u001b[K     |████████████████████████████████| 1.1MB 50.9MB/s \n","\u001b[?25hCollecting sacremoses\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n","\u001b[K     |████████████████████████████████| 890kB 51.7MB/s \n","\u001b[?25hRequirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.0.0) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.0.0) (2020.12.5)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.0.0) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.0.0) (1.24.3)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers==3.0.0) (2.4.7)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==3.0.0) (1.15.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==3.0.0) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==3.0.0) (0.17.0)\n","Building wheels for collected packages: sacremoses\n","  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893261 sha256=24ee1b97f0668ec13b52a32691b54c5f9256d90eb69b0f0973b0787e59d9e450\n","  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n","Successfully built sacremoses\n","Installing collected packages: tokenizers, sentencepiece, sacremoses, transformers\n","Successfully installed sacremoses-0.0.43 sentencepiece-0.1.94 tokenizers-0.8.0rc4 transformers-3.0.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"fL589BZSny29","executionInfo":{"status":"ok","timestamp":1607995645106,"user_tz":240,"elapsed":12523,"user":{"displayName":"Samin Fakharian","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjFi1gkXRQ0jRQiMMLiILHLVk8ulyIY9ehIbVkQ5g=s64","userId":"00770828924006406210"}}},"source":["import random\n","import os\n","import re\n","import math\n","import time\n","import datetime\n","import numpy as np\n","seed_val = 42\n","random.seed(seed_val)\n","np.random.seed(seed_val)\n","os.environ['PYTHONHASHSEED'] = str(seed_val)\n","import pylab as pl\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","import gensim\n","from gensim.models.word2vec import Word2Vec\n","from gensim.models import Word2Vec\n","\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","from collections import Counter, defaultdict\n","\n","from sklearn import utils\n","from sklearn.pipeline import Pipeline\n","from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.svm import SVC\n","from sklearn.model_selection import GridSearchCV\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.utils.class_weight import compute_class_weight\n","from sklearn.model_selection import train_test_split,learning_curve, GridSearchCV, cross_val_score, StratifiedKFold,StratifiedShuffleSplit\n","from sklearn.metrics import classification_report,accuracy_score\n","\n","import torch\n","torch.manual_seed(seed_val)\n","torch.cuda.manual_seed_all(seed_val)\n","torch.backends.cudnn.deterministic = True\n","torch.backends.cudnn.benchmark = False\n","import torch.nn as nn\n","from torch.utils.data import TensorDataset, random_split\n","from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n","\n","from tqdm import tqdm\n","tqdm.pandas(desc=\"progress-bar\")\n","\n","import transformers as tf\n","from transformers import get_linear_schedule_with_warmup\n","from transformers import AdamW\n","from transformers import AutoModel, BertTokenizerFast,BertModel\n","from transformers import RobertaTokenizer, RobertaModel, AdamW, get_linear_schedule_with_warmup\n","%matplotlib inline\n","\n","from google.colab import drive\n","from google.colab import files"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"1JltYaDSny3D","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1607995645113,"user_tz":240,"elapsed":12521,"user":{"displayName":"Samin Fakharian","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjFi1gkXRQ0jRQiMMLiILHLVk8ulyIY9ehIbVkQ5g=s64","userId":"00770828924006406210"}},"outputId":"ce5110a6-9134-4141-e78f-732270ce1190"},"source":["import torch\n","seed_val = 42\n","torch.manual_seed(seed_val)\n","torch.cuda.manual_seed_all(seed_val)\n","torch.backends.cudnn.deterministic = True\n","torch.backends.cudnn.benchmark = False\n","import torch.nn as nn\n","from torch.utils.data import TensorDataset, random_split\n","from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n","\n","# If there's a GPU available...\n","if torch.cuda.is_available():    \n","\n","    # Tell PyTorch to use the GPU.    \n","    device = torch.device(\"cuda\")\n","\n","    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n","\n","    print('We will use the GPU:', torch.cuda.get_device_name(0))\n","\n","# If not...\n","else:\n","    print('No GPU available, using the CPU instead.')\n","    device = torch.device(\"cpu\")"],"execution_count":3,"outputs":[{"output_type":"stream","text":["There are 1 GPU(s) available.\n","We will use the GPU: Tesla V100-SXM2-16GB\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"PoHbFBfs-GYR","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1607995671422,"user_tz":240,"elapsed":38816,"user":{"displayName":"Samin Fakharian","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjFi1gkXRQ0jRQiMMLiILHLVk8ulyIY9ehIbVkQ5g=s64","userId":"00770828924006406210"}},"outputId":"27dc0ba1-0d21-4e86-9698-359647dc9d42"},"source":["drive.mount('/content/drive')"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"NU8FrRmEny3J","executionInfo":{"status":"ok","timestamp":1607995675957,"user_tz":240,"elapsed":43341,"user":{"displayName":"Samin Fakharian","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjFi1gkXRQ0jRQiMMLiILHLVk8ulyIY9ehIbVkQ5g=s64","userId":"00770828924006406210"}}},"source":["dev_train=pd.read_csv('./drive/My Drive/Colab Notebooks/dataset/DEV-Train.csv')\n","dev_test=pd.read_csv('./drive/My Drive/Colab Notebooks/dataset/DEV-Test.csv')\n","test_train=pd.read_csv('./drive/My Drive/Colab Notebooks/dataset/TEST-Train.csv')\n","test_test=pd.read_csv('./drive/My Drive/Colab Notebooks/dataset/TEST-Test.csv')"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"4-CPccOZny3O","executionInfo":{"status":"ok","timestamp":1607995675996,"user_tz":240,"elapsed":43372,"user":{"displayName":"Samin Fakharian","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjFi1gkXRQ0jRQiMMLiILHLVk8ulyIY9ehIbVkQ5g=s64","userId":"00770828924006406210"}}},"source":["dev_train['label'] = dev_train.label.map({'L':0, 'I':1})\n","dev_train['isCanon'] = dev_train.isCanon.map({'Not':0, 'C':1})"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"i8goSr0tny3p","executionInfo":{"status":"ok","timestamp":1607995676004,"user_tz":240,"elapsed":43372,"user":{"displayName":"Samin Fakharian","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjFi1gkXRQ0jRQiMMLiILHLVk8ulyIY9ehIbVkQ5g=s64","userId":"00770828924006406210"}}},"source":["dev_test['label'] = dev_test.label.map({'L':0, 'I':1})\n","dev_test['isCanon'] = dev_test.isCanon.map({'Not':0, 'C':1})"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"qn89U3Jzny33","executionInfo":{"status":"ok","timestamp":1607995676017,"user_tz":240,"elapsed":43378,"user":{"displayName":"Samin Fakharian","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjFi1gkXRQ0jRQiMMLiILHLVk8ulyIY9ehIbVkQ5g=s64","userId":"00770828924006406210"}}},"source":["test_train['label'] = test_train.label.map({'L':0, 'I':1})\n","test_train['isCanon'] = test_train.isCanon.map({'Not':0, 'C':1})"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"uZ5GdSwzny4D","executionInfo":{"status":"ok","timestamp":1607995676022,"user_tz":240,"elapsed":43375,"user":{"displayName":"Samin Fakharian","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjFi1gkXRQ0jRQiMMLiILHLVk8ulyIY9ehIbVkQ5g=s64","userId":"00770828924006406210"}}},"source":["test_test['label'] = test_test.label.map({'L':0, 'I':1})\n","test_test['isCanon'] = test_test.isCanon.map({'Not':0, 'C':1})"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"9AAKrfqKA-9i","colab":{"base_uri":"https://localhost:8080/","height":204},"executionInfo":{"status":"ok","timestamp":1607995676062,"user_tz":240,"elapsed":43405,"user":{"displayName":"Samin Fakharian","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjFi1gkXRQ0jRQiMMLiILHLVk8ulyIY9ehIbVkQ5g=s64","userId":"00770828924006406210"}},"outputId":"b985ab86-9e3e-4598-ddec-c1c679178c49"},"source":["# dev_train[['verb','noun']] = dev_train['mwe'].str.split('_',expand=True)\n","# dev_test[['verb','noun']] = dev_test['mwe'].str.split('_',expand=True)\n","# test_train[['verb','noun']] = test_train['mwe'].str.split('_',expand=True)\n","# test_test[['verb','noun']] = test_test['mwe'].str.split('_',expand=True)\n","dev_train.head()"],"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>sentence</th>\n","      <th>fileName</th>\n","      <th>mwe</th>\n","      <th>label</th>\n","      <th>pattNum</th>\n","      <th>isCanon</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>To them I 'll say you 'll find things are stra...</td>\n","      <td>0</td>\n","      <td>find_foot</td>\n","      <td>1</td>\n","      <td>9</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>When called upon to tell us of the exemplary c...</td>\n","      <td>0</td>\n","      <td>blow_trumpet</td>\n","      <td>1</td>\n","      <td>5</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Taking heart from another record-setting sessi...</td>\n","      <td>0</td>\n","      <td>take_heart</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>It was there for all to see BR became less of ...</td>\n","      <td>0</td>\n","      <td>find_foot</td>\n","      <td>1</td>\n","      <td>9</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>One day much to Dustin 's irritation she asked...</td>\n","      <td>0</td>\n","      <td>see_star</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                            sentence  ...  isCanon\n","0  To them I 'll say you 'll find things are stra...  ...        1\n","1  When called upon to tell us of the exemplary c...  ...        1\n","2  Taking heart from another record-setting sessi...  ...        1\n","3  It was there for all to see BR became less of ...  ...        1\n","4  One day much to Dustin 's irritation she asked...  ...        0\n","\n","[5 rows x 6 columns]"]},"metadata":{"tags":[]},"execution_count":10}]},{"cell_type":"code","metadata":{"id":"Akd77Vamny4H","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1607995676064,"user_tz":240,"elapsed":43386,"user":{"displayName":"Samin Fakharian","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjFi1gkXRQ0jRQiMMLiILHLVk8ulyIY9ehIbVkQ5g=s64","userId":"00770828924006406210"}},"outputId":"6e61489c-c2a2-4c73-8b07-ab97f44e071b"},"source":["file_name=dev_train.fileName.unique().tolist()\n","file_name"],"execution_count":11,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]"]},"metadata":{"tags":[]},"execution_count":11}]},{"cell_type":"code","metadata":{"id":"TGh4HgXEDsiF","executionInfo":{"status":"ok","timestamp":1607995676067,"user_tz":240,"elapsed":43379,"user":{"displayName":"Samin Fakharian","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjFi1gkXRQ0jRQiMMLiILHLVk8ulyIY9ehIbVkQ5g=s64","userId":"00770828924006406210"}}},"source":["dev_train=test_train\n","dev_test=test_test"],"execution_count":12,"outputs":[]},{"cell_type":"code","metadata":{"id":"GWiZX4cJlA46","executionInfo":{"status":"ok","timestamp":1607995676069,"user_tz":240,"elapsed":43373,"user":{"displayName":"Samin Fakharian","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjFi1gkXRQ0jRQiMMLiILHLVk8ulyIY9ehIbVkQ5g=s64","userId":"00770828924006406210"}}},"source":["# dev_train=dev_train[dev_train.fileName==0]\n","# dev_test=dev_test[dev_test.fileName==0]"],"execution_count":13,"outputs":[]},{"cell_type":"code","metadata":{"id":"9Ntl98yvlYFB","executionInfo":{"status":"ok","timestamp":1607995676071,"user_tz":240,"elapsed":43368,"user":{"displayName":"Samin Fakharian","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjFi1gkXRQ0jRQiMMLiILHLVk8ulyIY9ehIbVkQ5g=s64","userId":"00770828924006406210"}}},"source":["def format_time(elapsed):\n","    '''\n","    Takes a time in seconds and returns a string hh:mm:ss\n","    '''\n","    # Round to the nearest second.\n","    elapsed_rounded = int(round((elapsed)))\n","    \n","    # Format as hh:mm:ss\n","    return str(datetime.timedelta(seconds=elapsed_rounded))\n"],"execution_count":14,"outputs":[]},{"cell_type":"code","metadata":{"id":"nx_1bhFlsVzB","executionInfo":{"status":"ok","timestamp":1607995676073,"user_tz":240,"elapsed":43362,"user":{"displayName":"Samin Fakharian","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjFi1gkXRQ0jRQiMMLiILHLVk8ulyIY9ehIbVkQ5g=s64","userId":"00770828924006406210"}}},"source":["# Function to calculate the accuracy of our predictions vs labels\n","def flat_accuracy(preds, labels):\n","    # pred_flat = np.argmax(preds, axis=1).flatten()\n","    # labels_flat = labels.flatten()\n","    return np.sum(preds == labels.numpy()) / len(labels)\n","\n","def flat_accuracy_num(preds, labels):\n","    # pred_flat = np.argmax(preds, axis=1).flatten()\n","    # labels_flat = labels.flatten()\n","    return np.sum(preds == labels.numpy())"],"execution_count":15,"outputs":[]},{"cell_type":"code","metadata":{"id":"732Ql8JxRvrm","executionInfo":{"status":"ok","timestamp":1607995676076,"user_tz":240,"elapsed":43358,"user":{"displayName":"Samin Fakharian","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjFi1gkXRQ0jRQiMMLiILHLVk8ulyIY9ehIbVkQ5g=s64","userId":"00770828924006406210"}}},"source":["# t=dev_train\n","# new_t=t.apply(find_word,axis=1)\n","# new_t"],"execution_count":16,"outputs":[]},{"cell_type":"code","metadata":{"id":"qY7j05_ToYW7","executionInfo":{"status":"ok","timestamp":1607995676079,"user_tz":240,"elapsed":43353,"user":{"displayName":"Samin Fakharian","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjFi1gkXRQ0jRQiMMLiILHLVk8ulyIY9ehIbVkQ5g=s64","userId":"00770828924006406210"}}},"source":["# new_t.to_excel('test2.xlsx')\n","# files.download('test2.xlsx') "],"execution_count":17,"outputs":[]},{"cell_type":"code","metadata":{"id":"SahR-sZygfYF","executionInfo":{"status":"ok","timestamp":1607995676080,"user_tz":240,"elapsed":43347,"user":{"displayName":"Samin Fakharian","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjFi1gkXRQ0jRQiMMLiILHLVk8ulyIY9ehIbVkQ5g=s64","userId":"00770828924006406210"}}},"source":["class Classifier(nn.Module):\n","\n","    def __init__(self, roberta):\n","      \n","        super(Classifier, self).__init__()\n","\n","        self.roberta = RobertaModel.from_pretrained('roberta-base',output_hidden_states=True)\n","      \n","        self.dropout = nn.Dropout(0.5)\n","        # relu activation function\n","        self.relu =  nn.ReLU()\n","        # dense layer 1\n","        self.fc1 = nn.Linear(768,512)\n","        # dense layer 2 (Output layer)\n","        self.fc2 = nn.Linear(512,2)\n","        #softmax activation function\n","        self.softmax = nn.LogSoftmax(dim=1)\n","\n","    #define the forward pass\n","    def forward(self, sent_id, mask):\n","      #pass the inputs to the model  \n","        # _, cls_hs = self.roberta(input_ids=sent_id, attention_mask=mask)\n","\n","        outputs= self.roberta(input_ids=sent_id, attention_mask=mask)\n","        # cls_hs=outputs[0][:,0,:]\n","        outputs = outputs[2]\n","        cls_hs=outputs[l_num][:,0,:]\n","        x = self.fc1(cls_hs)\n","        x = self.relu(x)\n","        x = self.dropout(x)\n","        # output layer\n","        x = self.fc2(x)\n","        # apply softmax activation\n","        x = self.softmax(x)\n","\n","        return x"],"execution_count":18,"outputs":[]},{"cell_type":"code","metadata":{"id":"41XaVN4ng6sj","executionInfo":{"status":"ok","timestamp":1607995676082,"user_tz":240,"elapsed":43340,"user":{"displayName":"Samin Fakharian","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjFi1gkXRQ0jRQiMMLiILHLVk8ulyIY9ehIbVkQ5g=s64","userId":"00770828924006406210"}}},"source":["# function to train the model\n","def train():\n","  \n","  model.train()\n","#   i=0\n","  total_loss, total_accuracy = 0, 0\n","  \n","  # empty list to save model predictions\n","  total_preds=[]\n","  \n","  # iterate over batches\n","  for step,batch in enumerate(train_dataloader):\n","    # print(i)\n","    # progress update after every 50 batches.\n","    if step % 10 == 0 and not step == 0:\n","      print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(train_dataloader)))\n","\n","    # push the batch to gpu\n","    batch = [r.to(device) for r in batch]\n"," \n","    sent_id, mask, labels = batch\n","\n","    # clear previously calculated gradients \n","    model.zero_grad()        \n","\n","    # get model predictions for the current batch\n","    preds = model(sent_id, mask)\n","\n","    # compute the loss between actual and predicted values\n","    loss = cross_entropy(preds, labels)\n","\n","    # add on to the total loss\n","    total_loss = total_loss + loss.item()\n","\n","    # backward pass to calculate the gradients\n","    loss.backward()\n","\n","    # clip the the gradients to 1.0. It helps in preventing the exploding gradient problem\n","    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","    # scheduler = get_linear_schedule_with_warmup(optimizer, \n","    #                                                     num_warmup_steps = 0, # Default value in run_glue.py\n","    #                                                     num_training_steps = total_steps)\n","    # update parameters\n","    optimizer.step()\n","    # scheduler.step()\n","    # model predictions are stored on GPU. So, push it to CPU\n","    preds=preds.detach().cpu().numpy()\n","\n","    # append the model predictions\n","    total_preds.append(preds)\n","    # i+=1\n","  # compute the training loss of the epoch\n","  avg_loss = total_loss / len(train_dataloader)\n","  \n","  # predictions are in the form of (no. of batches, size of batch, no. of classes).\n","  # reshape the predictions in form of (number of samples, no. of classes)\n","  total_preds  = np.concatenate(total_preds, axis=0)\n","\n","  #returns the loss and predictions\n","  return avg_loss, total_preds"],"execution_count":19,"outputs":[]},{"cell_type":"code","metadata":{"id":"oOW3mvSig6vy","executionInfo":{"status":"ok","timestamp":1607995676084,"user_tz":240,"elapsed":43334,"user":{"displayName":"Samin Fakharian","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjFi1gkXRQ0jRQiMMLiILHLVk8ulyIY9ehIbVkQ5g=s64","userId":"00770828924006406210"}}},"source":["# function for evaluating the model\n","def evaluate():\n","  \n","  print(\"\\nEvaluating...\")\n","  \n","  # deactivate dropout layers\n","  model.eval()\n","#   i=0  \n","  total_loss, total_accuracy = 0, 0\n","  \n","  # empty list to save the model predictions\n","  total_preds = []\n","\n","  # iterate over batches\n","  for step,batch in enumerate(val_dataloader):\n","    # print(i)\n","    # Progress update every 50 batches.\n","    if step % 10 == 0 and not step == 0:\n","      \n","      # Calculate elapsed time in minutes.\n","    #   elapsed = format_time(time.time() - t0)\n","            \n","      # Report progress.\n","      print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(val_dataloader)))\n","\n","    # push the batch to gpu\n","    batch = [t.to(device) for t in batch]\n","\n","    sent_id, mask, labels = batch\n","\n","    # deactivate autograd\n","    with torch.no_grad():\n","    #   print(\"val_loop:\",i)\n","      # model predictions\n","      preds = model(sent_id, mask)\n","\n","      # compute the validation loss between actual and predicted values\n","      loss = cross_entropy(preds,labels)\n","\n","      total_loss = total_loss + loss.item()\n","\n","      preds = preds.detach().cpu().numpy()\n","\n","      total_preds.append(preds)\n","    # i+=1\n","  # compute the validation loss of the epoch\n","  avg_loss = total_loss / len(val_dataloader) \n","\n","  # reshape the predictions in form of (number of samples, no. of classes)\n","  total_preds  = np.concatenate(total_preds, axis=0)\n","\n","  return avg_loss, total_preds"],"execution_count":20,"outputs":[]},{"cell_type":"code","metadata":{"id":"mOOfNXjeAvBP","executionInfo":{"status":"ok","timestamp":1607995676293,"user_tz":240,"elapsed":43536,"user":{"displayName":"Samin Fakharian","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjFi1gkXRQ0jRQiMMLiILHLVk8ulyIY9ehIbVkQ5g=s64","userId":"00770828924006406210"}}},"source":[""],"execution_count":20,"outputs":[]},{"cell_type":"code","metadata":{"id":"Z3eI0_m3dqFL","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1608005331979,"user_tz":240,"elapsed":275680,"user":{"displayName":"Samin Fakharian","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjFi1gkXRQ0jRQiMMLiILHLVk8ulyIY9ehIbVkQ5g=s64","userId":"00770828924006406210"}},"outputId":"25dffa9a-89d0-4664-9ae9-645a10ae0ebf"},"source":["###################################################################################\n","#####################################all together###################################\n","##################################################################################\n","temp_train=pd.DataFrame()\n","temp_validation=pd.DataFrame()\n","train_loss=[]\n","lr_clf_scores_dict={}\n","svc_clf_scores_dict={}\n","avg_acc=[]\n","val_result = []\n","# learning_rate_range=[2e-5,3e-5,5e-5]\n","# epoch_range=[4,2,3]\n","# batch_size_range=[16,8,32]\n","learning_rate_range=[3e-5]\n","epoch_range=[4]\n","batch_size_range=[32]\n","l_num=0\n","i=0\n","run=0\n","final_result_df=pd.DataFrame()\n","final_result_df_le=pd.DataFrame()\n","random_seeds=[68,96,20,53,91,72,42,14,24,67]\n","for explored_layer in range(2,3): #13\n","    for rand_var in random_seeds:\n","        for er in epoch_range:\n","          for lrr in learning_rate_range:\n","            for bsr in batch_size_range:\n","                for ds in range(9,10):\n","                    l_num = 0 - explored_layer\n","                    layer = 13 - explored_layer\n","                    print(str(run))\n","                    print(\"------------------------------------------------------------\")\n","                    run+=1\n","                    print(\"Start processing for: \",ds)\n","                    temp_train=dev_train[dev_train.fileName==ds]\n","                    temp_test=dev_test[dev_test.fileName==ds]\n","\n","                    seed_value = rand_var\n","                    # random.seed(seed_val)\n","                    # np.random.seed(seed_val)\n","                    # torch.manual_seed(seed_val)\n","                    # torch.cuda.manual_seed_all(seed_val)\n","                    import os\n","                    os.environ['PYTHONHASHSEED']=str(seed_value)\n","                    # 2. Set `python` built-in pseudo-random generator at a fixed value\n","                    import random\n","                    random.seed(seed_value)\n","                    # 3. Set `numpy` pseudo-random generator at a fixed value\n","                    import numpy as np\n","                    np.random.seed(seed_value)\n","                    import torch\n","                    torch.manual_seed(seed_value)\n","                    torch.cuda.manual_seed_all(seed_value)\n","                    torch.backends.cudnn.deterministic = True\n","                    torch.backends.cudnn.benchmark = False\n","\n","                    # roberta = RobertaModel.from_pretrained('roberta-base')\n","                    roberta = RobertaModel.from_pretrained('roberta-base',output_hidden_states=True)\n","                    # Load the RoBERTa tokenizer\n","                    tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n","                    # get length of all the messages in the train set\n","                    seq_len = [len(i.split()) for i in dev_train['sentence']]\n","                    # pd.Series(seq_len).hist(bins = 30)\n","                    max_seq_len = 0\n","                    max_len = max_seq_len\n","                    # For every sentence...\n","                    for sent in temp_train['sentence']:\n","                        # Tokenize the text and add `[CLS]` and `[SEP]` tokens.\n","                        train_input_ids = tokenizer.encode(sent, add_special_tokens=True)\n","                        # Update the maximum sentence length.\n","                        max_seq_len = max(max_seq_len, len(train_input_ids))\n","                    # print(max_seq_len)\n","                    train_text, val_text, train_labels, val_labels = train_test_split(temp_train['sentence'], temp_train['label'], \n","                                                                            random_state=2018, \n","                                                                            test_size=0.2, \n","                                                                            stratify=temp_train['label'])\n","\n","                    # we will use temp_text and temp_labels to create validation and test set\n","                    test_text =temp_test['sentence']\n","                    test_labels =temp_test['label']\n","                    # tokenize and encode sequences in the training set\n","                    tokens_train = tokenizer.batch_encode_plus(\n","                        train_text.tolist(),\n","                        max_length = max_seq_len,\n","                        pad_to_max_length=True,\n","                        truncation=True,\n","                        return_token_type_ids=False\n","                    )\n","\n","                    # tokenize and encode sequences in the validation set\n","                    tokens_val = tokenizer.batch_encode_plus(\n","                        val_text.tolist(),\n","                        max_length = max_seq_len,\n","                        pad_to_max_length=True,\n","                        truncation=True,\n","                        return_token_type_ids=False\n","                    )\n","\n","                    # tokenize and encode sequences in the test set\n","                    tokens_test = tokenizer.batch_encode_plus(\n","                        test_text.tolist(),\n","                        max_length = max_seq_len,\n","                        pad_to_max_length=True,\n","                        truncation=True,\n","                        return_token_type_ids=False\n","                    )\n","                    # for train set\n","                    train_seq = torch.tensor(tokens_train['input_ids'])\n","                    train_mask = torch.tensor(tokens_train['attention_mask'])\n","                    train_y = torch.tensor(train_labels.tolist())\n","\n","                    # for validation set\n","                    val_seq = torch.tensor(tokens_val['input_ids'])\n","                    val_mask = torch.tensor(tokens_val['attention_mask'])\n","                    val_y = torch.tensor(val_labels.tolist())\n","\n","                    # for test set\n","                    test_seq = torch.tensor(tokens_test['input_ids'])\n","                    test_mask = torch.tensor(tokens_test['attention_mask'])\n","                    test_y = torch.tensor(test_labels.tolist())\n","\n","                    #define a batch size\n","                    batch_size = bsr\n","                    # batch_size = 32\n","\n","                    # wrap tensors\n","                    train_data = TensorDataset(train_seq, train_mask, train_y)\n","\n","                    # sampler for sampling the data during training\n","                    train_sampler = RandomSampler(train_data)\n","\n","                    # dataLoader for train set\n","                    train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n","\n","                    # wrap tensors\n","                    val_data = TensorDataset(val_seq, val_mask, val_y)\n","\n","                    # sampler for sampling the data during training\n","                    val_sampler = SequentialSampler(val_data)\n","\n","                    # dataLoader for validation set\n","                    val_dataloader = DataLoader(val_data, sampler = val_sampler, batch_size=batch_size)\n","                    CF_arr=torch.tensor(temp_train.isCanon.to_numpy())\n","                    CF_arr=torch.reshape(CF_arr,(-1,1))\n","                    CF_arr=CF_arr.to(device)\n","                    CF_test_arr=torch.tensor(temp_test.isCanon.to_numpy())\n","                    CF_test_arr=torch.reshape(CF_test_arr,(-1,1))\n","                    CF_test_arr=CF_arr.to(device)\n","\n","                    training_stats = []\n","                    training_stats_le = []\n","                    # pass the pre-trained BERT to our define architecture\n","                    model = Classifier(roberta)\n","\n","                    # push the model to GPU\n","                    model = model.to(device)\n","                    # optimizer from hugging face transformer\n","                    # define the optimizer\n","                    optimizer = AdamW(model.parameters(), lr = lrr)\n","\n","                    #compute the class weights\n","                    class_wts = compute_class_weight('balanced', np.unique(train_labels), train_labels)\n","                    # convert class weights to tensor\n","                    weights= torch.tensor(class_wts,dtype=torch.float)\n","                    weights = weights.to(device)\n","\n","                    # loss function\n","                    cross_entropy  = nn.NLLLoss(weight=weights) \n","\n","                    # number of training epochs\n","                    epochs = er\n","                    # set initial loss to infinite\n","                    best_valid_loss = float('inf')\n","\n","                    # empty lists to store training and validation loss of each epoch\n","                    train_losses=[]\n","                    valid_losses=[]\n","\n","                    #for each epoch\n","                    for epoch in range(epochs):\n","                        \n","                        print('\\n Epoch {:} / {:}'.format(epoch + 1, epochs))\n","                        \n","                        #train model\n","                        train_loss, _ = train()\n","                        \n","                        #evaluate model\n","                        valid_loss, _ = evaluate()\n","                        \n","                        #save the best model\n","                        if valid_loss < best_valid_loss:\n","                            best_valid_loss = valid_loss\n","                            torch.save(model.state_dict(), 'saved_weights.pt')\n","                        \n","                        # append training and validation loss\n","                        train_losses.append(train_loss)\n","                        valid_losses.append(valid_loss)\n","                        \n","                        print(f'\\nTraining Loss: {train_loss:.3f}')\n","                        print(f'Validation Loss: {valid_loss:.3f}')\n","                        \n","                    # get predictions for test data\n","                    with torch.no_grad():\n","                        preds = model(test_seq.to(device), test_mask.to(device))\n","                        preds = preds.detach().cpu().numpy()\n","                    # model's performance\n","                    preds = np.argmax(preds, axis = 1)\n","\n","                    # print(classification_report(test_y, preds))\n","                    total_eval_accuracy_num = 0\n","                    total_eval_accuracy = 0\n","                    total_eval_accuracy = flat_accuracy(preds, test_y)\n","                    total_eval_accuracy_num = flat_accuracy_num(preds, test_y)\n","                    print(\"Number of True Prediction: {0:10.4f}\".format(total_eval_accuracy_num))\n","                    # avg_val_accuracy = total_eval_accuracy / len(test_y)\n","                    print(\"Accuracy: {0:10.4f}\".format(total_eval_accuracy))\n","                    val_result.append((layer,rand_var,total_eval_accuracy, total_eval_accuracy_num, len(test_y), ds, lrr, er, bsr))\n","                    print('DONE.')\n","validation_df=pd.DataFrame(val_result,columns=['layer','rand_var','accuracy', 'true_labels', 'all_labels','file', 'lr', 'e', 'b'])\n","# validation_df.to_excel(str(layer)+'-RoBERTa-AE-FT-DNoCF-CLS-TEST.xlsx',index=False)\n","# files.download(str(layer)+'-RoBERTa-AE-FT-DNoCF-CLS-TEST.xlsx')\n","validation_df.to_excel(str(layer)+'-RoBERTa-AE-FT-DNoCF-CLS-TEST.xlsx',index=False)\n","files.download(str(layer)+'-RoBERTa-AE-FT-DNoCF-CLS-TEST.xlsx')"],"execution_count":29,"outputs":[{"output_type":"stream","text":["0\n","------------------------------------------------------------\n","Start processing for:  9\n","\n"," Epoch 1 / 4\n","  Batch    10  of     12.\n","\n","Evaluating...\n","\n","Training Loss: 0.703\n","Validation Loss: 0.692\n","\n"," Epoch 2 / 4\n","  Batch    10  of     12.\n","\n","Evaluating...\n","\n","Training Loss: 0.708\n","Validation Loss: 0.683\n","\n"," Epoch 3 / 4\n","  Batch    10  of     12.\n","\n","Evaluating...\n","\n","Training Loss: 0.620\n","Validation Loss: 0.431\n","\n"," Epoch 4 / 4\n","  Batch    10  of     12.\n","\n","Evaluating...\n","\n","Training Loss: 0.353\n","Validation Loss: 0.389\n","Number of True Prediction:   114.0000\n","Accuracy:     0.7972\n","DONE.\n","1\n","------------------------------------------------------------\n","Start processing for:  9\n","\n"," Epoch 1 / 4\n","  Batch    10  of     12.\n","\n","Evaluating...\n","\n","Training Loss: 0.710\n","Validation Loss: 0.691\n","\n"," Epoch 2 / 4\n","  Batch    10  of     12.\n","\n","Evaluating...\n","\n","Training Loss: 0.691\n","Validation Loss: 0.687\n","\n"," Epoch 3 / 4\n","  Batch    10  of     12.\n","\n","Evaluating...\n","\n","Training Loss: 0.670\n","Validation Loss: 0.568\n","\n"," Epoch 4 / 4\n","  Batch    10  of     12.\n","\n","Evaluating...\n","\n","Training Loss: 0.438\n","Validation Loss: 0.439\n","Number of True Prediction:   107.0000\n","Accuracy:     0.7483\n","DONE.\n","2\n","------------------------------------------------------------\n","Start processing for:  9\n","\n"," Epoch 1 / 4\n","  Batch    10  of     12.\n","\n","Evaluating...\n","\n","Training Loss: 0.723\n","Validation Loss: 0.695\n","\n"," Epoch 2 / 4\n","  Batch    10  of     12.\n","\n","Evaluating...\n","\n","Training Loss: 0.703\n","Validation Loss: 0.680\n","\n"," Epoch 3 / 4\n","  Batch    10  of     12.\n","\n","Evaluating...\n","\n","Training Loss: 0.608\n","Validation Loss: 0.436\n","\n"," Epoch 4 / 4\n","  Batch    10  of     12.\n","\n","Evaluating...\n","\n","Training Loss: 0.312\n","Validation Loss: 0.348\n","Number of True Prediction:   126.0000\n","Accuracy:     0.8811\n","DONE.\n","3\n","------------------------------------------------------------\n","Start processing for:  9\n","\n"," Epoch 1 / 4\n","  Batch    10  of     12.\n","\n","Evaluating...\n","\n","Training Loss: 0.717\n","Validation Loss: 0.687\n","\n"," Epoch 2 / 4\n","  Batch    10  of     12.\n","\n","Evaluating...\n","\n","Training Loss: 0.676\n","Validation Loss: 0.554\n","\n"," Epoch 3 / 4\n","  Batch    10  of     12.\n","\n","Evaluating...\n","\n","Training Loss: 0.417\n","Validation Loss: 0.294\n","\n"," Epoch 4 / 4\n","  Batch    10  of     12.\n","\n","Evaluating...\n","\n","Training Loss: 0.253\n","Validation Loss: 0.270\n","Number of True Prediction:   125.0000\n","Accuracy:     0.8741\n","DONE.\n","4\n","------------------------------------------------------------\n","Start processing for:  9\n","\n"," Epoch 1 / 4\n","  Batch    10  of     12.\n","\n","Evaluating...\n","\n","Training Loss: 0.729\n","Validation Loss: 0.687\n","\n"," Epoch 2 / 4\n","  Batch    10  of     12.\n","\n","Evaluating...\n","\n","Training Loss: 0.659\n","Validation Loss: 0.544\n","\n"," Epoch 3 / 4\n","  Batch    10  of     12.\n","\n","Evaluating...\n","\n","Training Loss: 0.401\n","Validation Loss: 0.445\n","\n"," Epoch 4 / 4\n","  Batch    10  of     12.\n","\n","Evaluating...\n","\n","Training Loss: 0.226\n","Validation Loss: 0.332\n","Number of True Prediction:   126.0000\n","Accuracy:     0.8811\n","DONE.\n","5\n","------------------------------------------------------------\n","Start processing for:  9\n","\n"," Epoch 1 / 4\n","  Batch    10  of     12.\n","\n","Evaluating...\n","\n","Training Loss: 0.730\n","Validation Loss: 0.691\n","\n"," Epoch 2 / 4\n","  Batch    10  of     12.\n","\n","Evaluating...\n","\n","Training Loss: 0.698\n","Validation Loss: 0.666\n","\n"," Epoch 3 / 4\n","  Batch    10  of     12.\n","\n","Evaluating...\n","\n","Training Loss: 0.587\n","Validation Loss: 0.457\n","\n"," Epoch 4 / 4\n","  Batch    10  of     12.\n","\n","Evaluating...\n","\n","Training Loss: 0.397\n","Validation Loss: 0.368\n","Number of True Prediction:   118.0000\n","Accuracy:     0.8252\n","DONE.\n","6\n","------------------------------------------------------------\n","Start processing for:  9\n","\n"," Epoch 1 / 4\n","  Batch    10  of     12.\n","\n","Evaluating...\n","\n","Training Loss: 0.708\n","Validation Loss: 0.690\n","\n"," Epoch 2 / 4\n","  Batch    10  of     12.\n","\n","Evaluating...\n","\n","Training Loss: 0.687\n","Validation Loss: 0.675\n","\n"," Epoch 3 / 4\n","  Batch    10  of     12.\n","\n","Evaluating...\n","\n","Training Loss: 0.602\n","Validation Loss: 0.398\n","\n"," Epoch 4 / 4\n","  Batch    10  of     12.\n","\n","Evaluating...\n","\n","Training Loss: 0.387\n","Validation Loss: 0.347\n","Number of True Prediction:   119.0000\n","Accuracy:     0.8322\n","DONE.\n","7\n","------------------------------------------------------------\n","Start processing for:  9\n","\n"," Epoch 1 / 4\n","  Batch    10  of     12.\n","\n","Evaluating...\n","\n","Training Loss: 0.706\n","Validation Loss: 0.692\n","\n"," Epoch 2 / 4\n","  Batch    10  of     12.\n","\n","Evaluating...\n","\n","Training Loss: 0.703\n","Validation Loss: 0.671\n","\n"," Epoch 3 / 4\n","  Batch    10  of     12.\n","\n","Evaluating...\n","\n","Training Loss: 0.568\n","Validation Loss: 0.318\n","\n"," Epoch 4 / 4\n","  Batch    10  of     12.\n","\n","Evaluating...\n","\n","Training Loss: 0.312\n","Validation Loss: 0.278\n","Number of True Prediction:   119.0000\n","Accuracy:     0.8322\n","DONE.\n","8\n","------------------------------------------------------------\n","Start processing for:  9\n","\n"," Epoch 1 / 4\n","  Batch    10  of     12.\n","\n","Evaluating...\n","\n","Training Loss: 0.707\n","Validation Loss: 0.691\n","\n"," Epoch 2 / 4\n","  Batch    10  of     12.\n","\n","Evaluating...\n","\n","Training Loss: 0.701\n","Validation Loss: 0.676\n","\n"," Epoch 3 / 4\n","  Batch    10  of     12.\n","\n","Evaluating...\n","\n","Training Loss: 0.598\n","Validation Loss: 0.390\n","\n"," Epoch 4 / 4\n","  Batch    10  of     12.\n","\n","Evaluating...\n","\n","Training Loss: 0.270\n","Validation Loss: 0.432\n","Number of True Prediction:   119.0000\n","Accuracy:     0.8322\n","DONE.\n","9\n","------------------------------------------------------------\n","Start processing for:  9\n","\n"," Epoch 1 / 4\n","  Batch    10  of     12.\n","\n","Evaluating...\n","\n","Training Loss: 0.711\n","Validation Loss: 0.692\n","\n"," Epoch 2 / 4\n","  Batch    10  of     12.\n","\n","Evaluating...\n","\n","Training Loss: 0.674\n","Validation Loss: 0.634\n","\n"," Epoch 3 / 4\n","  Batch    10  of     12.\n","\n","Evaluating...\n","\n","Training Loss: 0.452\n","Validation Loss: 0.331\n","\n"," Epoch 4 / 4\n","  Batch    10  of     12.\n","\n","Evaluating...\n","\n","Training Loss: 0.324\n","Validation Loss: 0.256\n","Number of True Prediction:   125.0000\n","Accuracy:     0.8741\n","DONE.\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/javascript":["\n","    async function download(id, filename, size) {\n","      if (!google.colab.kernel.accessAllowed) {\n","        return;\n","      }\n","      const div = document.createElement('div');\n","      const label = document.createElement('label');\n","      label.textContent = `Downloading \"${filename}\": `;\n","      div.appendChild(label);\n","      const progress = document.createElement('progress');\n","      progress.max = size;\n","      div.appendChild(progress);\n","      document.body.appendChild(div);\n","\n","      const buffers = [];\n","      let downloaded = 0;\n","\n","      const channel = await google.colab.kernel.comms.open(id);\n","      // Send a message to notify the kernel that we're ready.\n","      channel.send({})\n","\n","      for await (const message of channel.messages) {\n","        // Send a message to notify the kernel that we're ready.\n","        channel.send({})\n","        if (message.buffers) {\n","          for (const buffer of message.buffers) {\n","            buffers.push(buffer);\n","            downloaded += buffer.byteLength;\n","            progress.value = downloaded;\n","          }\n","        }\n","      }\n","      const blob = new Blob(buffers, {type: 'application/binary'});\n","      const a = document.createElement('a');\n","      a.href = window.URL.createObjectURL(blob);\n","      a.download = filename;\n","      div.appendChild(a);\n","      a.click();\n","      div.remove();\n","    }\n","  "],"text/plain":["<IPython.core.display.Javascript object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"application/javascript":["download(\"download_3933a7ac-fd4d-4359-afef-79912c564b95\", \"11-RoBERTa-AE-FT-DNoCF-CLS-TEST.xlsx\", 5424)"],"text/plain":["<IPython.core.display.Javascript object>"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"00z59zUdttSB","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1608005331983,"user_tz":240,"elapsed":272836,"user":{"displayName":"Samin Fakharian","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjFi1gkXRQ0jRQiMMLiILHLVk8ulyIY9ehIbVkQ5g=s64","userId":"00770828924006406210"}},"outputId":"846b641f-30c9-4254-eeec-ece71d85f96c"},"source":["validation_df"],"execution_count":30,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>layer</th>\n","      <th>rand_var</th>\n","      <th>accuracy</th>\n","      <th>true_labels</th>\n","      <th>all_labels</th>\n","      <th>file</th>\n","      <th>lr</th>\n","      <th>e</th>\n","      <th>b</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>11</td>\n","      <td>68</td>\n","      <td>0.797203</td>\n","      <td>114</td>\n","      <td>143</td>\n","      <td>9</td>\n","      <td>0.00003</td>\n","      <td>4</td>\n","      <td>32</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>11</td>\n","      <td>96</td>\n","      <td>0.748252</td>\n","      <td>107</td>\n","      <td>143</td>\n","      <td>9</td>\n","      <td>0.00003</td>\n","      <td>4</td>\n","      <td>32</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>11</td>\n","      <td>20</td>\n","      <td>0.881119</td>\n","      <td>126</td>\n","      <td>143</td>\n","      <td>9</td>\n","      <td>0.00003</td>\n","      <td>4</td>\n","      <td>32</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>11</td>\n","      <td>53</td>\n","      <td>0.874126</td>\n","      <td>125</td>\n","      <td>143</td>\n","      <td>9</td>\n","      <td>0.00003</td>\n","      <td>4</td>\n","      <td>32</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>11</td>\n","      <td>91</td>\n","      <td>0.881119</td>\n","      <td>126</td>\n","      <td>143</td>\n","      <td>9</td>\n","      <td>0.00003</td>\n","      <td>4</td>\n","      <td>32</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>11</td>\n","      <td>72</td>\n","      <td>0.825175</td>\n","      <td>118</td>\n","      <td>143</td>\n","      <td>9</td>\n","      <td>0.00003</td>\n","      <td>4</td>\n","      <td>32</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>11</td>\n","      <td>42</td>\n","      <td>0.832168</td>\n","      <td>119</td>\n","      <td>143</td>\n","      <td>9</td>\n","      <td>0.00003</td>\n","      <td>4</td>\n","      <td>32</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>11</td>\n","      <td>14</td>\n","      <td>0.832168</td>\n","      <td>119</td>\n","      <td>143</td>\n","      <td>9</td>\n","      <td>0.00003</td>\n","      <td>4</td>\n","      <td>32</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>11</td>\n","      <td>24</td>\n","      <td>0.832168</td>\n","      <td>119</td>\n","      <td>143</td>\n","      <td>9</td>\n","      <td>0.00003</td>\n","      <td>4</td>\n","      <td>32</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>11</td>\n","      <td>67</td>\n","      <td>0.874126</td>\n","      <td>125</td>\n","      <td>143</td>\n","      <td>9</td>\n","      <td>0.00003</td>\n","      <td>4</td>\n","      <td>32</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   layer  rand_var  accuracy  true_labels  all_labels  file       lr  e   b\n","0     11        68  0.797203          114         143     9  0.00003  4  32\n","1     11        96  0.748252          107         143     9  0.00003  4  32\n","2     11        20  0.881119          126         143     9  0.00003  4  32\n","3     11        53  0.874126          125         143     9  0.00003  4  32\n","4     11        91  0.881119          126         143     9  0.00003  4  32\n","5     11        72  0.825175          118         143     9  0.00003  4  32\n","6     11        42  0.832168          119         143     9  0.00003  4  32\n","7     11        14  0.832168          119         143     9  0.00003  4  32\n","8     11        24  0.832168          119         143     9  0.00003  4  32\n","9     11        67  0.874126          125         143     9  0.00003  4  32"]},"metadata":{"tags":[]},"execution_count":30}]},{"cell_type":"code","metadata":{"id":"QRaOgxvxsLw_","executionInfo":{"status":"ok","timestamp":1607995961720,"user_tz":240,"elapsed":328921,"user":{"displayName":"Samin Fakharian","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjFi1gkXRQ0jRQiMMLiILHLVk8ulyIY9ehIbVkQ5g=s64","userId":"00770828924006406210"}}},"source":[""],"execution_count":22,"outputs":[]}]}